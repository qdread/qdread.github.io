Changing the way science is done: I'm thinking of two things here. The first is to get USDA scientists to move away from the old paradigm of yes-or-no answers to statistical questions. For historically obscure reasons, the number 95% was enshrined as the gold standard for whether a phenomenon you observe is "real" or not. In other words, if we are 95% (or more) confident\* that, let's say, a drug reduces cancer rate in lab rats, or a new crop rotation technique increases corn yield, or whatever, we can say "drug x causes significant reduction in cancer" or "new crop rotation significantly increases corn yield," but if we are only 94% confident based on our data, we can't say that. This is an illogical and absurd way of doing science. There's no such thing, in reality, as a black-and-white distinction between "yes it has an effect" and "no it doesn't." Not only that, because publishing your scientific study is more likely when you have a "significant" result (the official word for 95% or greater confidence), scientists are incentivized to fiddle around with their statistical analyses until they get something 95% or greater, then only publish those results. This leads to a lot of spurious results getting published because you can often get a result like that from luck alone, and also leads to lots of interesting data that can't be forced to produce 95% confidence in an effect getting discarded so that no one can use and learn from it. This has led to a situation called the "reproducibility crisis" 


\* : This is a real oversimplification of what the 95% number actually means but I think that being slightly imprecise is OK here to get the point across.